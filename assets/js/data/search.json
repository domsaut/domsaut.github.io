[ { "title": "Sum of N Dice", "url": "/posts/Sum-N-Dice/", "categories": "Python", "tags": "probability, python", "date": "2022-01-29 16:09:00 +1300", "snippet": "Today, I am going to explore a recursive solution to a fascinating probability problem- what is the probability that the sum of $n$ dice is $m$? I will present the probabilistic background used, followed by the recursive solution’s implementation in Python.The problem at handRecall that the problem is defined as the following:What is the probability that the sum of n dice is m?I find that when dealing with general probability questions like this, it is always a good idea to start with the base case and build upwards. For the sum of 2 dice, we can construct a table with $[1,6]$ on the vertical and horizontal axis respectively, and fill it in with the sums of the intersecting numbers. There will be $6^2$ possible outcomes, and we can count the number that sum to our required $m$ value. We can extend this idea to 3 dice, by extending the table out into 3 dimensions, but the this is a rather labour intensive process that doesn’t scale very well further than this. We need a better idea to move forward with…A lovely recursive statementConsider the following scenario again. We want to sum 2d6 dice, with random variables $X_1$ and $X_2$ respectively, to get some outcome $Y$. It is clear that the random variable $Y$ is defined as follows: $Y = X_1 + X_2$. Due to this relationship, we can compute the probability mass function for $Y$:\\[P_Y(y) = P(X_1 = 1)P(X_2 = y - 1) + P(X_1 = 2)P(X_2 = y - 2) + P(X_1 = 3)P(X_2 = y - 3) + \\dots + P(X_1 = 6)P(X_2 = y - 6)\\]Using this idea, we can write the pmf for the sum of 3 dice, $Z$, as the following: $Z = X_1 + X_2 + X_3$, or rather $Z = Y + X_3$. Similarly, the pmf can be written as:\\[P_Z(z) = P(X_3 = 1)P(Y = z - 1) + P(X_3 = 2)P(Y = z - 2) + P(X_3 = 3)P(Y = z - 3) + \\dots + P(X_3 = 6)P(Y = z - 6)\\]It is this recursive idea that we around going to exploit to generalise the problem for $n$ dice.Let’s Code the Relationship!The following Python code handles our problem nicely for relatively smaller n values (recursion isn’t always the most efficient solution):def funct(sum_d, n_dice): &#39;&#39;&#39; recursive function to find probability of n_dice summing to sum_d &#39;&#39;&#39; if n_dice == 1: if sum_d in range(1,7): return 1/6 else: return 0 else: return 1/6 * (funct(sum_d-1, n_dice-1) + funct(sum_d-2, n_dice-1) + funct(sum_d-3, n_dice-1) + funct(sum_d-4, n_dice-1) + funct(sum_d-5, n_dice-1) + funct(sum_d-6, n_dice-1) )Using this function, we can now generate the probabilities for any $n$ dice, within computational limits. Let’s generate the pmf for 2 dice:And for 3 dice:For 4 dice:If you’ve ever done any statistics before, alarm bells will likely be going off at the shape of this distribution as we add in more dice. Let’s explore this idea a little further in the next section.Central Limit TheoremIt appears that the distribution is converging on the a gaussian as we add in more coins. Let’s overlay a normal with matching variance and mean to explore this idea a little more.Seems like a pretty good fit already! Notice that some of the imperfection can be attributed to the precision loss due to the float datatype. If we take a look at 6 dice we can see the distribution matches the gaussian almost exactly.This is happening as a direct result of a famous statistical result called the Central Limit Theorem. The CLT states that in many cases, when independent random variables are summed up, their sum tends towards a normal distribution, even if the variables themselves are not normally distributed. We can observe this exact phenomenon occurring above in this scenario. Just for fun, let’s take a look at the pmf for 9 dice juxtaposed to the corresponding normal distribution:" }, { "title": "Statistics Dashboard for Arithmetic Zetamac", "url": "/posts/Zetamac-Statistics/", "categories": "Python", "tags": "programming, python", "date": "2021-12-18 16:09:00 +1300", "snippet": "In today&#39;s post, I step through how I developed a statistics dashboard for an online maths game Arithmetic Zetamac. Zetamac is a minimalist online speed maths game which, although serves its purpose very well, does not have a native statistics dashboard or way to store your previous scores. As someone interested in tracking progress over time, I would track my scores on a google doc- a very labour intensive task. Today we will see how to automate local data storage using Selenium and produce a dashboard showing statistics using PySimpleGui.The Zetamac ClassLet us commence our development process by defining a class named Zetamac which will encapsulate all of our program&#39;s logic. We need to make sure that we parse the url of the game-type we want, and the length of that game into the constructer when initialising the class.It is also important to check if the necessary infrastructure exists before diving into our code. Let&#39;s make sure we check that our data storage csv exists in the program directory. Let&#39;s also define our web driver- I used Chrome for no good reason.class Zetamac: def __init__(self, link: str, game_time: int)-&amp;gt; None: self.link = link self.game_time = game_time if not os.path.isfile(&#39;.\\data.csv&#39;): with open(&#39;.\\data.csv&#39;, &#39;w&#39;) as f: writer = csv.writer(f) header = [&#39;time&#39;,&#39;score&#39;] writer.writerow(header) f.close() # define our driver for Selenium web interaction self.driver = webdriver.Chrome(&#39;.\\chromedriver&#39;)Now that we have the base of our class established, let&#39;s start building in some of the features. For the purposes of this blog post, I am going to omit some of the less interesting methods like init_browser() and init_game(), but rather discuss which methods they point to. The complete code will be listed at the bottom of this post for those that are interested.Data Scraping and End of Game GUIThe init_game() method is called at the beginning of every game. This method simply executes time.sleep() for the duration of the game, and then calls the store_data() method followed by self.end_of_game(). Let’s take a look these methods.def store_data(self) -&amp;gt; int: &quot;&quot;&quot; Attempt to store data &quot;&quot;&quot; try: score = self.driver.find_element_by_xpath(&#39;//*[@id=&quot;game&quot;]/div/div[2]/p[1]&#39;).text.split()[-1] except IndexError: print(&#39;Score not in! Trying again in 2 seconds.&#39;) time.sleep(3) score = self.driver.find_element_by_xpath(&#39;//*[@id=&quot;game&quot;]/div/div[2]/p[1]&#39;).text.split()[-1] with open(&#39;.\\data.csv&#39;, &#39;a&#39;, newline = &#39;&#39;) as f: writer = csv.writer(f) data = [datetime.now(), score] writer.writerow(data) f.close() return scoreI am confident there is a more elegant way to deal with waiting the duration of the game, by checking for final establishment of the score for example, but I opted for a very quick and dirty method- terrible programming practice. This method uses Selenium to scrape the score from the website via its xpath. It is typical that, depending on site loading times, the script is executed early and an IndexError is thrown. This is handled with an exception allowing more time. After we get the score, it is timestamped and stored in the local csv.Let’s take a look at how the end_of_game popup is implemented in PySimpleGui:def end_of_game(self, score: int) -&amp;gt; None: &quot;&quot;&quot; GUI for end of game scenario &quot;&quot;&quot; sg.theme(&#39;DarkAmber&#39;) font = (&#39;Roboto Mono&#39;, 10) layout = [ [sg.Text(size=(1,1), key=&#39;-OUT-&#39;)], [sg.Text(&#39;Score: {}&#39;.format(score), font = (&#39;Roboto Mono&#39;, 16))], [sg.Text(&#39;Restarting in ...&#39;, key = &#39;-TEXT-&#39;)], [sg.Text(size=(1,1), key=&#39;-OUT-&#39;)], [sg.Button(&#39;Play Again&#39;), sg.Button(&#39;Stats&#39;), sg.Button(&#39;Exit&#39;)] ] window = sg.Window( &#39;Zetamac&#39;, layout, element_justification=&#39;c&#39;, size=(400, 175), font = font, finalize=True ) timer = Countdown(5) window.bring_to_front()The physical appearance of the GUI is organised within the layout array in rows. We can define columns too, which we will see how to do a little later in the stats dashboard implementation.A lot of button clicking between games can get a little tedious too so I implemented an auto-restart feature 5 seconds after a game is completed. Countdown is a separate class defined as follows:class Countdown: def __init__(self, seconds: int): self.target_time = int(time.time()) + seconds self.running = True def counting(self): &quot;&quot;&quot; Returns seconds until timer is complete &quot;&quot;&quot; time_remaining = max(self.target_time - int(time.time()), 0) if not time_remaining: self.running = False return time_remaining def status(self): &quot;&quot;&quot; Check if countdown has reached expiry &quot;&quot;&quot; return self.runningIn our case, we are using a 5 second countdown which is called within our event loop:# match indentation from end_of_game method while True: # event loop event, values = window.read(timeout = 10) if event in (sg.WIN_CLOSED ,&#39;Exit&#39;): self.driver.quit() window.close() break elif event == &#39;Play Again&#39; or not timer.status(): window.close() self.restart_game() break elif event == &#39;Stats&#39;: window.close() self.stats(score) break window[&#39;-TEXT-&#39;].update(&#39;Restarting in ... {}&#39;.format(timer.counting()))The window.update() call is what allows us to change the countdown on the screen for the user to see. This is what the end of game GUI with the specified theme looks like!Statistics DashboardThe main event of the program is the statistics dashboard. The calculation of these statistics is relatively boring, so we will instead focus on the implementation of the GUI. It is worth noting that the stats_calculation method outputs a tuple containing all of the statistics calculated from the data csv.One of the stand-out features of the dashboard is the plot. This is made in matplotlib with scores from the user&#39;s previous ten games. The line is smoothed using scipy’s interpolate.make_interp_spline. Disclaimer: I am aware that this sometimes results in scores that are negative or inflated in an effort to fit a nice curve. This is a purely aesthetic choice and there is nothing mathematical about it! :)The stats dashboard is implemented as follows:def stats(self, score: int) -&amp;gt; None: &quot;&quot;&quot; GUI for performance statistics dashboard &quot;&quot;&quot; statistics = self.stats_calculation() self.generate_stat_plot() sg.theme(&#39;DarkAmber&#39;) font = (&#39;Roboto Mono&#39;, 10) image = [[sg.Image(&#39;./plot.png&#39;)]] col = [ [sg.Text(&#39;score&#39;,font = (&#39;Roboto Mono&#39;, 14))], [sg.Text(score, font = (&#39;Roboto Mono&#39;, 22), text_color= &#39;white&#39;)], [sg.Text(size=(1,1), key=&#39;-OUT-&#39;)], [sg.Text(&#39;pb&#39;,font = (&#39;Roboto Mono&#39;, 14))], [sg.Text(statistics[0],font = (&#39;Roboto Mono&#39;, 22), text_color= &#39;white&#39;)] ] def mini_col(label, score): return sg.Column([[sg.Text(label, font = (&#39;Roboto Mono&#39;, 10))],[sg.Text(score, font = (&#39;Roboto Mono&#39;, 16), text_color= &#39;white&#39;)]]) layout = [ [sg.Column(col), sg.Column(image, element_justification=&#39;c&#39;)], [ mini_col(&#39;best today&#39;, statistics[1]), mini_col(&#39;av (last 10)&#39;,statistics[2]), mini_col(&#39;std (last 10)&#39;,statistics[3]), mini_col(&#39;time today&#39;,statistics[4]), mini_col(&#39;av (all time)&#39;,statistics[5]) ], [sg.Text(size = (1,1), key = &#39;-OUT-&#39;)], [sg.Button(&#39;Play Again&#39;), sg.Button(&#39;Exit&#39;)] ] window = sg.Window( &#39;Zetamac&#39;, layout, element_justification=&#39;c&#39;, size=(1200, 650), font = font ) event, values = window.read() if event == sg.WIN_CLOSED or event == &#39;Exit&#39;: window.close() self.driver.quit() if event == &#39;Play Again&#39;: window.close() self.restart_game()You can also see my incredibly messy column implementation here (please reach out if you have any suggestions for tidying up the code!). I have also implemented two buttons at the bottom which allow the user to exit or restart the game. The dashboard ended up looking like this:Here you can notice the wiggly effect of the fitted line.Concluding Thoughts and Full CodeOverall, in terms of maintainability, the code base is pretty poor. There are a lot of static parts that can break the code if the website is ever updated, and more generally a lot of places where things can go wrong. However, for my use case I think that is fine- this was a quick and dirty script to solve a simple problem after all.If you are interested in checking out the full code, I have a public repo linked here. I would love your feedback and ideas for improvements too. Contact details in the side bar!" }, { "title": "Bertrand&#39;s Box Paradox", "url": "/posts/Bertrand-Box/", "categories": "Probability", "tags": "probability, python", "date": "2021-12-14 16:36:00 +1300", "snippet": "Have you ever solved a somewhat trivial problem, but felt like the mathematics deceived you? In today’s blog post I plan on walking through a rather elementary probability question whose solution employs an application of Bayes Law. I will also include a simple simulation as an additional unrigorous verification of my calculated solution to this problem.Bertrand’s Box ParadoxThe Bertrand Box Paradox was first presented by Joseph Bertrand in 1889. The version of the problem we will be considering today is defined as follows:“There are three boxes on a table: The first box contains 2 quarters, the second box contains 2 nickels, and the third box contains 1 quarter and 1 nickel. One chooses a box at random, then chooses a coin from that box at random. Provided the chosen coin is a quarter, what’s the probability of the other coin being a quarter?”To solve this problem, we are looking for the probability of two quarters, conditioning on the fact that the first seen coin is a quarter. Expressed mathematically:\\[P(\\text{QQ | see Q})\\]Where: $P(QQ) = P(NN) = P(QN) = \\frac{1}{3}$The Naive ApproachAt first, it may be tempting to change the sample space to work out our probability. We know that we have observed a $Q$ on the first choice, therefore for our second choice, it may seem tempting to consider an updated sample space of {$QQ, QN$}. If this is the case, provided we observed a $Q$ on the first throw, we now have a sample space of {$Q, N$}, which are equally likely. This would suggest that the probability is in fact $\\frac{1}{2}$, which is the incorrect solution. The assumption that they are equally likely is where one may make the critical mistake. Recall that initially, we have the selection of each box as equally likely, but the probability of $QQ$ giving a $Q$ is $1$, and the probability of $QN$ giving a $Q$ is $0.5$, so we must consider the conditional probability to approach this correctly.Bayes Theorem and the Law of Total ProbabilityThis problem requires an application of Bayes Theorem, as defined below:\\[P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\\]In our case we have the following:\\[P(\\text{QQ | see Q}) = \\frac{P(\\text{QQ}) P(\\text{see Q | QQ})}{P(\\text{see Q})}\\]It is then convenient to expand the denominator out using the Law of Total Probability. The Law of Total Probability is a very useful theorem that states that for a set of pairwise disjoint events {$B_n : n = 1,2,3,…$} whose union makes up the whole sample space, any event $A$ of the same probability space can be expressed as the following$^1$:\\[P(A) = \\sum_n P(A|B_n)P(B_n)\\]We can make use of this law to express $P(\\text{see Q})$ as the following within our original expression:\\[\\frac{P(\\text{QQ}) P(\\text{see Q | QQ})}{P(\\text{see Q | QQ})P(\\text{QQ}) + P(\\text{see Q | NN})P(\\text{NN}) + P(\\text{see Q | QN})P(\\text{QN})}\\]Substituting we have the following:\\[= \\frac{\\frac{1}{3} \\times 1}{1 \\times \\frac{1}{3} + 0 \\times \\frac{1}{3} + \\frac{1}{2} \\times \\frac{1}{3}}\\]\\[= \\frac{2}{3}\\]Which is indeed, not $\\frac{1}{2}$.Python SimulationSometimes, I find it useful to verify my solution using a different method. Let us explore a simple way of simulating this problem in Python.Let us define our sample space using a 2d array.sample_space = [ [&#39;Q&#39;, &#39;Q&#39;], [&#39;Q&#39;, &#39;N&#39;], [&#39;N&#39;, &#39;N&#39;]]Let us construct our experimentdef experiment() -&amp;gt; str: # random.choice samples from a discrete uniform distribution box = random.choice(3) choice_in_box = random.choice(2) if sample_space[box][choice_in_box] == &#39;Q&#39;: # note that 1 - choice_in_box is just selecting the remaining coin return sample_space[box][1 - choice_in_box]def run_many_experiments(num_experiments): results = {&#39;Q&#39;: 0, &#39;N&#39;:0} for _ in range(num_experiments): output = experiment() if output: results[output] += 1 return resultsFinally, let us visualise the results of many trialsX = np.logspace(1,5, dtype = int, num = 50)Y = np.zeros(50)for i in range(len(X)): result = run_many_experiments(X[i]) prob = result[&#39;Q&#39;] / sum(result.values()) Y[i] = probplt.plot(X,Y,&#39;.-&#39;, color = &#39;orange&#39;)plt.xscale(&#39;log&#39;)plt.xlabel(&#39;Number of trials&#39;)plt.ylabel(&#39;Probability&#39;)plt.title(&#39;P(QQ| see Q on first)&#39;) Which produces the following:Which does indeed also converge to $\\frac{2}{3}$ as expected." }, { "title": "Solving the Black Scholes PDE", "url": "/posts/Solving-the-Black-Scholes-PDE/", "categories": "Options Pricing", "tags": "pde, options", "date": "2021-11-04 18:36:00 +1300", "snippet": "In this post, I intend to step through the Black Scholes (1973) options pricing model derivation from start to finish, in a complete and accessible way. In a previous post, I explored a way to derive the pricing model using stochastic calculus and risk neutral expectation. This time I will take a more ‘applied mathematics approach’ by deriving the Black Scholes PDE, transforming it into a more recognizable heat equation form, and solving it using Green’s functions.The Black Scholes EquationAs in previous posts, we will use the following stochastic model for stock price:\\[\\frac{dS_t}{S_t} = rdt + \\sigma dB_t\\]With $dS_t^2 = S_t^2 \\sigma^2 dt$ (a result of Ito’s Lemma).We are interested in constructing a risk-free delta hedged portfolio:\\[\\pi = c_t - \\Delta S\\]Where: The portfolio consists of one option $c_t$, and $\\Delta$ stock $\\Delta$ is equal to $\\frac{\\partial c_t}{\\partial S_t}$ $\\pi$ is the value of the portfolio\\[d \\pi = dc_t - \\frac{\\partial c_t}{\\partial S_t} dS_t\\]However, this portfolio is risk free, therefore must earn the risk free rate $r$:\\[d \\pi = r \\pi dt = r(c_t - \\frac{\\partial c_t}{\\partial S_t}S_t)dt\\]\\[\\therefore dc_t - \\frac{\\partial c_t}{\\partial S_t} dS_t = r(c_t - \\frac{\\partial c_t}{\\partial S_t}S_t)dt\\]Let us now construct an algebraic expression for the infinitessimal $dc_t$. We know that $c_t(S_t, t)$ is a function of $S_t$ and $t$, therefore we can employ a Taylor expansion:\\[dc_t = \\frac{\\partial c_t}{\\partial S_t} dS_t + \\frac{\\partial c_t}{\\partial t} dt + \\frac{1}{2} \\frac{\\partial^2 c_t}{\\partial S_t^2}dS_t^2\\]Substituting and cleaning up…\\[dc_t = (\\frac{\\partial c_t}{\\partial t} + rS_t \\frac{\\partial c_t}{\\partial S_t} + \\frac{1}{2} \\sigma^2 S_t^2 \\frac{\\partial^2 c_t}{\\partial S_t^2})dt + \\sigma S_t \\frac{\\partial c_t}{\\partial S_t} dB_t\\]Which we can then substitute into our equation for $d\\pi$:\\[(\\frac{\\partial c_t}{\\partial t} + rS_t \\frac{\\partial c_t}{\\partial S_t} + \\frac{1}{2} \\sigma^2 S_t^2 \\frac{\\partial^2 c_t}{\\partial S_t^2})dt + \\sigma S_t \\frac{\\partial c_t}{\\partial S_t} dB_t - \\frac{\\partial c_t}{\\partial S_t} (S_t r dt + S_t \\sigma dB_t) = r(c_t - \\frac{\\partial c_t}{\\partial S_t}S_t)dt\\]Cancelling and rearranging we arrive at the Black Scholes Equation:\\[\\frac{\\partial c_t}{\\partial S_t} + \\frac{1}{2} \\sigma^2 S_t^2 \\frac{\\partial^2 c_t}{\\partial S_t^2} + rS_t \\frac{\\partial c_t}{\\partial S_t} - r c_t = 0\\]With initial conditions: $ c(S_T, T) = \\max(S_T - K, 0) $Two Step TransformationWe are now ready to transform the Black Scholes PDE into a more workable heath equation. Firstly, it would be nice if we could deal with those pesky $r$ terms. In order to this, we will employ a forward method transformation. Our goal in doing this is to bring the reference point forward to $T$:\\[\\begin{cases}c^*(S^*, T) = e^{r\\tau}\\max(S - K, 0) \\\\S^* = Se^{r\\tau}\\end{cases}\\]Where: $\\tau = T - t$We now need to construct the respective partial derivatives in terms of the transformed variables:\\[\\frac{\\partial c}{\\partial t} = \\frac{\\partial c}{\\partial \\tau} \\frac{\\partial \\tau}{\\partial t} = -\\frac{\\partial c}{\\partial \\tau}\\]Employing the product rule and the multi-variate chain rule:\\[= -(-re^{-r\\tau} c^*(S^*, t) + e^{-r\\tau}(\\frac{\\partial c^*}{\\partial S^*}\\frac{\\partial S^*}{\\partial \\tau} + \\frac{\\partial c^*}{\\partial t} \\frac{\\partial t}{\\partial \\tau}))\\]Which, recalling the definition of the $c$ transform we previously did:\\[= rc - (\\frac{\\partial c^*}{\\partial S^*}rS^* - \\frac{\\partial c^*}{\\partial t})e^{-r\\tau}\\]Let us now shift our attention to the next term:\\[\\frac{\\partial c}{\\partial S} = \\frac{\\partial c}{\\partial S^*} \\frac{\\partial S^*}{\\partial S}\\]\\[= \\frac{\\partial c^*}{\\partial s^*} \\frac{\\partial c}{\\partial c^*} \\frac{\\partial S^*}{\\partial S} = \\frac{\\partial c}{\\partial S^*} e^{-r\\tau} e^{r\\tau}\\]\\[= \\frac{\\partial c}{\\partial s^*}\\]And finally, the second partial derivative term…\\[\\frac{\\partial^2 c}{\\partial S^2} = \\frac{\\partial}{\\partial S}(\\frac{\\partial c}{\\partial s^*})\\]\\[= \\frac{\\partial S^*}{\\partial S} \\frac{\\partial}{\\partial S^*}(\\frac{\\partial c}{\\partial S^*}) = e^{r\\tau} \\frac{\\partial^2 c^*}{\\partial S^{*2}}\\]Substituting and doing some light cosmetic adjustments we arrive at:\\[\\frac{\\partial c^*}{\\partial t} + \\frac{1}{2}\\sigma^2 (S^*)^2 \\frac{\\partial^2 c^*}{\\partial S^{*2}} = 0\\]With initial conditions:\\[c^*(S^*, T) = \\max(S^* - K, 0)\\]We are now ready to apply the second transformation (hence the name ‘two-step transformation’)! This next transformation is called a price-moneyness transformation:\\[\\begin{cases}S^* = Ke^{\\frac{1}{2}\\sigma^2 \\tau + x} \\\\c^*(S^*, t) = Kf(x, \\tau)\\end{cases}\\]Let us start with $\\frac{\\partial c^*}{\\partial t}$:\\[\\frac{\\partial c^*}{\\partial t} = \\frac{\\partial c^*}{\\partial \\tau}\\frac{\\partial \\tau}{\\partial t}\\]\\[= - \\frac{\\partial c^*}{\\partial \\tau} = -K (\\frac{\\partial f}{\\partial \\tau} + \\frac{\\partial f}{\\partial x} \\frac{\\partial x}{\\partial \\tau})\\]But we know that $\\frac{\\partial x}{\\partial \\tau} = - \\frac{1}{2} \\sigma^2$, therefore:\\[\\frac{\\partial c^*}{\\partial t} = -K (\\frac{\\partial f}{\\partial \\tau} - \\frac{1}{2} \\sigma^2 \\frac{\\partial f}{\\partial x})\\]Shifting our attention to $\\frac{\\partial c^{*}}{\\partial S^{*}}$:\\[\\frac{\\partial c^*}{\\partial S^*} = K(\\frac{\\partial f}{\\partial x}\\frac{\\partial x}{\\partial S^*})\\]\\[= \\frac{K}{S^*} \\frac{\\partial f}{\\partial x}\\]With another application of the partial derivative we get…\\[\\frac{\\partial^2 c^*}{\\partial S^{*2}} = \\frac{\\partial}{\\partial S^*} (\\frac{K}{S^*} \\frac{\\partial f}{\\partial x})\\]Applying the product rule:\\[= - \\frac{K}{S^{*2}} \\frac{\\partial f}{\\partial x} + \\frac{K}{S^*}\\frac{\\partial}{\\partial S^*} \\frac{\\partial f}{\\partial x}\\]If we notice that $\\frac{\\partial}{\\partial S^{*}} = \\frac{\\partial x}{\\partial S^{*}} \\frac{\\partial}{\\partial x}$, we can simplify the expression to:\\[\\frac{\\partial^2 c^*}{\\partial S^{*2}} =-\\frac{K}{S^{*2}}(\\frac{\\partial f}{\\partial x} - \\frac{\\partial^2 f}{\\partial x^2})\\]We are now ready to substitute in! If we know that:\\[\\frac{\\partial c^*}{\\partial t} + \\frac{1}{2}\\sigma^2 (S^*)^2 \\frac{\\partial^2 c^*}{\\partial S^{*2}} = 0\\]Then by substitution, the following is equivalent:\\[-K (\\frac{\\partial f}{\\partial \\tau} - \\frac{1}{2} \\sigma^2 \\frac{\\partial f}{\\partial x})-\\frac{1}{2}\\sigma^2 (S^*)^2 \\frac{K}{S^{*2}}(\\frac{\\partial f}{\\partial x} - \\frac{\\partial^2 f}{\\partial x^2})\\]\\[\\therefore \\frac{\\partial f}{\\partial \\tau} = \\frac{1}{2} \\sigma^2 \\frac{\\partial^2 f}{\\partial x^2}\\]The initial conditions must also be adjusted:\\[c^*(S^*, T) = \\max(S^* - K, 0) = kf(x, \\tau)\\]\\[\\therefore f(x, \\tau) = \\frac{1}{K} \\max(S^* - K, 0) = \\frac{1}{K} \\max(Ke^{\\frac{1}{2}\\sigma^2 \\tau + x} - K, 0)\\]\\[\\therefore f(x, 0) = \\max(e^x- 1, 0)\\]With this, we are ready to solve the heat equation.Solving the Heat Equation with Green’s FunctionsIn a previous post, I showed that we can express solutions to the heat equation as the following:\\[u(x,t) = \\int_{-\\infty}^\\infty f(x_0)G(x,t;x_0)dx_0\\]Where $f(x_0)$ is the initial condition function. It was also shown for our initial value problem we will make use of the following Green’s Function:\\[G(x,t;x_0) = \\frac{1}{\\sqrt{2\\pi\\sigma^2t}}e^{-\\frac{(x-x_0)^2}{2\\sigma^2t}}\\]Substituting our initial conditions we have:\\[u(x,t) = \\int_{-\\infty}^\\infty \\max(e^{x_0} -1,0) \\frac{1}{\\sqrt{2\\pi\\sigma^2t}}e^{-\\frac{(x-x_0)^2}{2\\sigma^2t}}dx_0\\]We can deal with the nasty bounds by adjusting our bounds:\\[= \\int_{0}^\\infty (e^{x_0} -1) \\frac{1}{\\sqrt{2\\pi\\sigma^2t}}e^{-\\frac{(x-x_0)^2}{2\\sigma^2t}}dx_0\\]Letting $u = \\frac{x - x_0}{\\sqrt{\\sigma^2 t}}$, and adjusting our bounds we now have:\\[\\int_{-\\infty}^{\\frac{x}{\\sqrt{\\sigma^2t}}}(e^{x - \\sqrt{\\sigma^2 t}u} - 1) \\frac{1}{\\sqrt{2 \\pi}} e^{-\\frac{u^2}{2}}du\\]We now need to complete the square for the first term of the integral, while noticing that the second term is simply the standard cumulative normal distribution.\\[e^x \\int_{-\\infty}^{\\frac{x}{\\sqrt{\\sigma^2t}}}\\frac{1}{\\sqrt{2 \\pi}} e^{x - \\sqrt{\\sigma^2 t}u} e^{-\\frac{u^2}{2}}du - N(\\frac{x}{\\sqrt{\\sigma^2 t}})\\]Focusing in on the first term, after completing the square we have:\\[e^{x + \\frac{\\sigma^2 t}{2}} \\int_{-\\infty}^{\\frac{x}{\\sqrt{\\sigma^2t}}}\\frac{1}{\\sqrt{2 \\pi}} e^{- \\frac{(u + \\sqrt{\\sigma^2 t})^2}{2}}du\\]Let us not perform another substitution. Let $\\gamma = u + \\sqrt{\\sigma^2 t}$…\\[e^{x + \\frac{\\sigma^2 t}{2}} \\int_{-\\infty}^{\\frac{x}{\\sqrt{\\sigma^2t}} + \\sqrt{\\sigma^2 t}}\\frac{1}{\\sqrt{2 \\pi}} e^{- \\frac{\\gamma^2}{2}}du\\]\\[= e^{x + \\frac{\\sigma^2 t}{2}} N(\\frac{x}{\\sqrt{\\sigma^2t}} + \\sqrt{\\sigma^2 t})\\]\\[\\therefore f(x,\\tau) = e^{x + \\frac{\\sigma^2 t}{2}} N(\\frac{x}{\\sqrt{\\sigma^2t}} + \\sqrt{\\sigma^2 t}) - N(\\frac{x}{\\sqrt{\\sigma^2 t}})\\]We know that $S = Ke^{\\frac{1}{2}\\sigma^2 \\tau + x}$…\\[\\therefore x = \\ln \\frac{S}{K} + (r - \\frac{\\sigma^2}{2})\\tau\\]Let us define $d_2 = \\frac{x}{\\sqrt{\\sigma^2t}}$ and $d_1 = d_2 + \\sigma \\sqrt{\\tau}$\\[\\therefore f(x, \\tau) = \\frac{S}{K} e^{r\\tau} N(d_1) - N(d_2)\\]However, we know that $c_t = Ke^{-r\\tau} f(x, \\tau)$…\\[\\therefore c_t = S_t N(d_1) - Ke^{-r\\tau} N(d_2)\\]Which is the solution to the Black Scholes Equation." }, { "title": "Green&#39;s Function and the Heat Equation", "url": "/posts/Greens-Function-and-Heat-Equation/", "categories": "Mathematical Physics", "tags": "pde, diracdelta", "date": "2021-11-03 18:36:00 +1300", "snippet": "The Heat Equation in one dimensional space is one of a handful of special partial differential equations that seem to pop up in a range of different applications within the mathematical physics, applied mathematics and finance disciplines respectively. In our case, it is defined as follows:\\[\\frac{\\partial u}{\\partial t} = \\alpha \\frac{\\partial^2 u}{\\partial x^2}\\]With initial conditions $u(x,0) = f(x)$.Where $u(x,t)$ represents temperature, and $\\alpha$ is a constant representing thermal diffusivity.In this blog post, I am going to explore the Green’s Function solution to the Heat Equation.The Dirac Delta FunctionBefore I introduce Green’s Function, it is important to have some background understanding of the Dirac Delta function. The Dirac Delta ($\\delta$) function is known as the unit pulse function. It can be defined as follows:\\[\\delta(x - x_0) =\\begin{cases}\\infty \\quad \\, x = x_0 \\\\0 \\quad \\, x \\neq x_0\\\\\\end{cases}\\]With the useful property that:\\[\\int_{-\\infty}^{\\infty} \\delta(x - x_0)dx = 1\\]You can think of the $\\delta$ function as a single infinite pulse, with zero value elsewhere. It is also interesting to consider the $\\delta$ function in terms of the limit as $\\sigma$ tends to $0$ of the non-standard normal distribution. This gives more intuition as to why the above integral is equal to one:\\[\\lim_{\\sigma \\to 0} \\bigg[\\frac{1}{\\sqrt{2\\pi \\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} \\bigg] =\\begin{cases}+\\infty \\quad \\, x = \\mu \\\\0 \\quad \\, \\text{otherwise} \\\\\\end{cases}\\]\\[=\\delta(x - \\mu)\\]With this knowledge, we are ready to explore Green’s Function.Green’s FunctionIn general Green’s Functions can be thought of as integral kernels that are useful for solving partial differential equations initial value problems. In our context, our Green’s Function is a solution to the following:\\[\\frac{\\partial G}{\\partial t} = \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 G}{\\partial x^2}\\]Subject to initial conditions: $G(x, 0) = \\delta(x- x_0)$.Thinking in terms of the Physics application, we can consider this partial differential equation (PDE) as a way of modelling the diffusion of heat along a one-dimensional rod of arbitrary length in both directions. The closed system is initialized with a finite heat source of infinite temperature at point $x_0$, which then diffuses outwards in both directions.It is known that the following is a solution to the PDE and the corresponding Green’s Function for the problem at hand:\\[G(x,t; x_0) = \\frac{1}{\\sqrt{2\\pi\\sigma^2 t}}e^{-\\frac{(x-x_0)^2}{2\\sigma^2 t}}\\]Which is a non-standard normal distribution with mean $x_0$ and variance $\\sigma^2 t$. This makes sense, as when $t = 0$, we arrive back at the $\\delta$ function which, as time progresses, flattens out into a more platykurtic function. Let us construct a visual representation of the solution with the following function parameters:sigma = 1x0 = 0curve(dnorm(x, x0, time), xlim=c(-4,4), ylim = c(0,2), bty = &quot;L&quot;, col = &quot;blue&quot;, xlab = &#39;Position (x)&#39;, ylab = &#39;Temperature (u)&#39;, lwd = 1.5)Notice the diffusion of heat from a centralised point at $x_0$, to an almost uniform distribution as time progresses.Generating Solutions to the PDE:Green’s Functions becomes useful when we consider them as a tool to solve initial value problems. It can be shown that the solution to the heat equation initial value problem is equivalent to the following integral:\\[u(x,t) = \\int_{-\\infty}^\\infty f(x_0)G(x,t;x_0)dx_0\\]Where $f(x)$ is the function defined at $t=0$ for our initial value problem.We will make use of this property in a future post to solve the Black Scholes Heat Equation." }, { "title": "Deriving the Black Scholes Pricing Formula", "url": "/posts/Black-Scholes-Derivation/", "categories": "Options Pricing", "tags": "options, trading", "date": "2021-10-10 18:36:00 +1300", "snippet": "The Black-Scholes options pricing formula (Black &amp;amp; Scholes, 1973) is one of the most profound results in financial derivative pricing history. In today’s post, I am going to demonstrate a way to derive the price of a European call option using risk-neutral conditional expectation in $Q$-measure.The Black-Scholes formula is a solution to the following partial differential equation:\\[\\frac{\\partial c}{\\partial t} + \\frac{1}{2} \\sigma^2 S^2 \\frac{\\partial^2 c}{\\partial S^2} + r S \\frac{\\partial c}{\\partial S} - rc = 0\\]Which is known as the Black-Scholes Equation. This can be derived by constructing a Delta-Hedged option portfolio, but that is a problem for another post.Stochastic Process for Stock PriceIn Black and Scholes (1973), stock price is modeled using the following stochastic process:\\[dS_t = r S_t dt + \\sigma S_t dB_t\\]Which can be thought of the sum of a deterministic drift term and a stochastic noise term. Note that if we were working in $P$-measure, the $r$ term (risk-free rate) would be replaced with $\\mu$ (expected return). $B_t$ is standard Brownian motion or a Wiener process, with $dB_t$ being normally distributed with mean zero and variance $dt$.Let us commence converting from differential form to a slightly more useful integral form. First, we will make use of a key result from Ito’s calculus for expressing the infinitessimal of any stochastic process $X_t(S_t, t)$:\\[dX_t = \\partial_S X_t dS_t + \\partial_t X_t dS_t + \\frac{1}{2} \\partial^2_S X_t (dS_t)^2\\]\\[(dS_t)^2 = (r S_t dt)^2 + 2(r S_t dt)(\\sigma S_t dB_t) + (\\sigma S_t dB_t)^2\\]However, the first and second term vanish as $dt^2$ and $dtdB_t$ are sufficiently small. By Ito’s Lemma, the final term becomes:\\[= \\sigma^2 S_t^2 dt\\]Let our stochastic process $X_t = \\ln S_t$:\\[d(\\ln S_t) = \\frac{1}{S_t} dS_t + 0 - \\frac{1}{2} \\frac{1}{S_t^2} \\sigma^2 S_t^2 dt\\]Substituting and cancelling:\\[= (r - \\frac{1}{2}\\sigma^2)dt + \\sigma dB_t\\]Integrating both sides from $t$ to $T$ and letting $\\tau \\equiv T - t$:\\[\\int_t^T d\\ln S_u = \\int_t^T (r- \\frac{1}{2} \\sigma^2) du + \\int_t^T \\sigma dB_u\\]\\[\\ln(\\frac{S_T}{S_t}) = (r - \\frac{1}{2}\\sigma^2)\\tau + \\sigma dB_\\tau\\]Exponentiating and rearranging:\\[\\fbox{$S_T = S_t e^{(r - \\frac{1}{2}\\sigma^2)\\tau + \\sigma B_\\tau} $}\\]Deriving the Price of a European CallAt expiry $T$, the pay-off function of a call option is defined as follows:\\[c_T = \\text{max}(S_T - K, 0)\\]Where $K$ is the strike price of the option. What we require is the value of the option at time $t$, prior to expiry. We can arive at an analytic function for $c_t$ by employing the conditional expectation of $c_T$ in $Q$-measure and discounting back to the present time.\\[c_t = e^{-r\\tau} E_t^Q (c_T)\\]To compute this, we will need to make use of the integral definition of expectation.\\[E[X] = \\int_\\mathbb{R} x f(x) dx\\]\\[\\therefore c_t = e^{-r\\tau}\\int_{-\\infty}^\\infty \\text{max}(S_T - K, 0)\\frac{1}{\\sqrt{2\\pi\\tau}} e^{- \\frac{B_\\tau^2}{2\\tau}} dB_\\tau\\]However, before proceeding we will need to deal with that nasty max function. We can do this by computing for which values of $B_\\tau$ will $S_T - K$ drop below 0:\\[S_T - K &amp;gt; 0\\]\\[S_t e^{(r - \\frac{1}{2}\\sigma^2)\\tau + \\sigma B_\\tau} - K &amp;gt; 0\\]After some rearranging and cosmetic adjustments:\\[B_\\tau &amp;gt; - \\frac{\\ln(\\frac{S_t}{K}) + (r - \\frac{\\sigma^2}{2})\\tau}{\\sigma \\sqrt{\\tau}} \\sqrt{\\tau}\\]Which we will define as:\\[B_\\tau &amp;gt; -d_2 \\sqrt{\\tau}\\]Now, our expectation can be simplified to the following:\\[E_t^Q (c_T) = \\int_{-d_2\\sqrt{\\tau}}^\\infty S_t e^{(r - \\frac{\\sigma^2}{2})\\tau + \\sigma B_\\tau} \\frac{1}{\\sqrt{2\\pi\\tau}} e^{- \\frac{B_\\tau^2}{2\\tau}} dB_\\tau - K \\int_{-d_2\\sqrt{\\tau}}^\\infty \\frac{1}{\\sqrt{2\\pi\\tau}} e^{- \\frac{B_\\tau^2}{2\\tau}} dB_\\tau\\]Which we will break up into 2 problems as follows:\\[:= \\text{I} - \\text{II}\\]Dealing with $\\text{I}$ first:\\[\\text{I} = S_t e^{(r - \\frac{\\sigma^2}{2})\\tau} \\int_{-d^2\\sqrt{\\tau}}^\\infty \\frac{e^{\\sigma B_\\tau}e^{\\frac{-B_\\tau^2}{2\\tau}}}{\\sqrt{2\\pi\\tau}} dB_\\tau\\]Completing the square so we can perform intergration:\\[= S_t e^{(r - \\frac{\\sigma^2}{2})\\tau} e^{\\frac{\\sigma^2 \\tau}{2}} \\int_{-d_2\\sqrt{\\tau}}^\\infty \\frac{e^{\\frac{(B_\\tau - t\\sigma)^2}{2t}}}{ \\sqrt{2\\pi\\tau}} dB_\\tau\\]We are now ready to perform a substitution. Let $y = -\\frac{B_\\tau - t\\sigma}{\\sqrt{t}}$.When $B_\\tau = \\infty: y = -\\infty$, and when $B_\\tau = -d_2\\sqrt{\\tau}: y = d_2 + \\sqrt{\\tau}\\sigma := d_1$. Substituting and flipping bounds:\\[=S_t e^{r\\tau} \\int_{-\\infty}^{d_1} \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{y^2}{2}} dy\\]Which is the definition of the cumulative normal density function $N(X)$.\\[\\therefore \\text{I} = S_t e^{r\\tau}N(d_1)\\]Shifting our attention to take a look at $\\text{II}$:\\[\\text{II} = K \\int_{-d_2\\sqrt{\\tau}}^\\infty \\frac{1}{\\sqrt{2\\pi\\tau}} e^{- \\frac{B_\\tau^2}{2\\tau}} dB_\\tau\\]Which can be dealt with using the substitution $y = -\\frac{B_\\tau}{\\sqrt{\\tau}}$. Substituting and swapping bounds we arive at:\\[= K\\int_{-\\infty}^{d_2} \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{y^2}{2}} dy = K\\cdot N(d_2)\\]Finally, plugging all of this back into our initial equation we get:\\[c_t = e^{-r\\tau} (S_t e^{r\\tau}N(d_1) - K\\cdot N(d_2))\\]\\[\\therefore \\fbox{$c_t = S_t N(d_1) - Ke^{-r\\tau}N(d_2)$}\\]Where $d_1 = \\frac{\\ln(\\frac{S_t}{K}) + (r + \\frac{\\sigma^2}{2})\\tau}{\\sigma \\sqrt{\\tau}}$ and $d_2 = \\frac{\\ln(\\frac{S_t}{K}) + (r - \\frac{\\sigma^2}{2})\\tau}{\\sigma \\sqrt{\\tau}}$.Corresponding Put Option Price from Put-Call ParityWe know that at terminal time $T$, the following relationship must hold by the no-arbitrage principle:\\[c_T - p_T = S_T - K\\]In order to discount this back to the present time $t$, we must once again take the discounted conditional expectation in $Q$-measure:\\[e^{-r\\tau} E_t^Q(c_T - p_T) = e^{-r\\tau}E_t^Q(S_T) - Ke^{-r\\tau}\\]\\[c_t - p_t = e^{-r\\tau}E_t^Q(S_t e^{(r - \\frac{1}{2}\\sigma^2)\\tau + \\sigma B_\\tau}) - Ke^{-r\\tau}\\]\\[= S_t - Ke^{-r\\tau}\\]Rearranging for $p_t$:\\[p_t = Ke^{-r\\tau}(1 - N(d_2)) - S_t(1 - N(d_1))\\]But, by the symmetry of the normal cumulative density function:\\[N(-x) + N(x) = 1\\]\\[\\therefore 1 - N(x) = N(-x)\\]Therefore we can simplify the expression to the following:\\[\\therefore \\fbox{$p_t = Ke^{-r\\tau} N(-d_2) - S_tN(-d_1)$}\\]" }, { "title": "Momentum Trading in Cryptocurrencies", "url": "/posts/Momentum-Trading-In-Crypto/", "categories": "Algorithmic Trading", "tags": "crypto, trading", "date": "2021-10-10 02:52:00 +1300", "snippet": "Momentum trading is a technical trading strategy in which one analyses short-term price data, buys assets that have been showing upwards-trends, and closes positions when that trend starts to lose momentum. In this strategy, the exponential moving average of price is used as a technical indicator to develop trading signals.Exponential Moving AverageWhen looking backwards at historical prices, traders have a range of different moving averages to choose from. The most common of which is a simple moving average (SMA). The SMA is, as the name suggests, is an equally weighted average calculation.The primary drawback of using a simple moving average is that new information is considered to be equally as important as older information. In reality, this is not the case and it is likely that, by the efficient market hypothesis, the most recent price (excluding any noise) contains the most up-to-date information and should therefore be considered the most important. To capture this idea, we employ an exponential moving average as defined below:\\[\\text{EMA}_t(P, \\alpha) =\\begin{cases}P_0, &amp;amp; t = 0 \\\\t\\alpha \\cdot P_t + (1 - \\alpha) \\cdot \\text{EMA}_{t-1} (P, \\alpha), &amp;amp; t &amp;gt; 0\\end{cases}\\]For this bot, the tuning parameter $\\alpha$ was defined using the its centre of mass definition:\\[\\alpha = \\frac{1}{1 + \\text{com}}\\]Below, we highlight the differences in the EMAs when changing the COM hyper-parameter from 8 to 24. Notice the differences in reactivity to movements in price.Generating a Trading signalThe signal is then created as follows:ema_l = lookback[&#39;close&#39;].ewm(com = n_l, adjust = False).mean()[today]ema_s = lookback[&#39;close&#39;].ewm(com = n_s, adjust = False).mean()[today]sigma_s = lookback[&#39;close&#39;].rolling(window = sigma_s_lookback).std()[today]y = (ema_s - ema_l) / sigma_ssignal_values.append(y)z = y / np.std(signal_values)In essence, the difference of the long and short term EMA is being used to determine potential entrance and exit points. This difference is then normalised using the volatility of the underlying, before being further normalised using the standard deviation of the signal itself. The outputted z value however is not standardised and is therefore not particularly useful to us. Let us then standardise z such that it will take values between -1 and 1 inclusive.u = (z * np.exp(-z**2 /4)) / (np.sqrt(2) * np.exp(-1/2))Notice that we actually transform the signal twice- the first time with a short term volatility measure of the underlying, and the second time with a longer term volatility measure of the signal itself. Due to this we actually lose a lot of trading signals in the warm-up period while we wait for the data to accumulate. This can be seen in the plot below. After all of our transformations, we can make use of the signal detailed below. It is standardised to be between -1 and 1, where positive values indicate a long position and negative numbers indicate a short position, and the magnitude represents the relative strength of that signal. This algorithm however, only assumes long positions.Now, it is important to construct a threshold for which we buy and sell. This should be subject to optimisation, but I chose to bin the signal into 3 equally sized responses- “long”, “short” and “neutral”. The signal outputs a long position when the signal is greater than 0.33, and short position when the signal is less than 0.33. Let us define this threshold as $\\eta$. Note that in order to make the algorithm more aggressive, one would simply need to widen the window for buying and selling. We will explore this idea below.Testing the AlgorithmLet us examine the results of the algorithm on Cardano (ADA) using high frequency 30-minute price data, and start tweaking some of the algorithm hyper-parameters. Firstly, let’s use default values as defined below. Note that these values all have units of ‘30 minute periods’, except $\\eta$, which has no units. $n_s$ $n_l$ $\\hat \\sigma(P)$ $\\hat \\sigma(y_k)$ $\\eta$ 8 24 12 168 0.33 Executing the back-test over a one-year period:Notice the high degree of correlation between price and return- this is slightly worrying for all-weather performance. The high degree of correlation may suggest that this algorithm will out-perform when the underlying is out-performing, but under-perform in a bear market.Let’s now explore a more aggressive algorithm with a $\\eta$ of 0.1 for demonstration purposes.Notice that this strategy performs a lot worse- likely a result of more false trading signals. We also trade 326 times- much more than our previous strategy. Maintaining all other variables as constant and increasing our $\\eta$ value to 0.5 will yield opposite results:Changing the $\\eta$ value to 0.5, we can observe that the algorithm makes 220 trades. In general, it seems that the more conservative $\\eta$ value seems to yield better trading decisions, but may not exit positions fast enough to secure profits.Opportunities for Further ExplorationThe keen readers may have noticed that the chosen hyper-parameter values for $n_s$ and $n_l$ did not receive proper justification. One may choose to fit these values to historical back-test results in the form of a multivariate grid search or alike. However, this poses a rather significant risk of over-fitting to historical conditions. If irrespective of this, we did optimise the values, what exactly we would optimise for poses an interesting question. Optimising for return alone would likely lead to a highly risk tolerant algorithm. Sharpe ratio may be a more appropriate proxy for performance.The primary issue with this algorithm is the coin-specific risk as a result of trading a single Crypto- in our case Cardano. This should be dealt with by implementing a portfolio of coins. Fortunately, this strategy does scale well to that idea and signals can be generated simultaneously on multiple coins if desired, with cash allocation simply evenly split between assets." } ]
